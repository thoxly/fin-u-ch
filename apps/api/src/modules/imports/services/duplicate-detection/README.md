# Duplicate Detection System

Система обнаружения дубликатов операций при импорте банковских выписок. Использует комбинацию хэширования операций и fuzzy matching для выявления уже существующих записей. Проверка выполняется в двух таблицах: `imported_operations` (необработанные черновики) и `operations` (уже импортированные операции).

Хэширование операций выполняется через функцию `createOperationHash` из shared пакета, которая создает уникальный хэш на основе ключевых полей: дата операции, сумма, назначение платежа (первые 200 символов), и опционально номера счетов. Хэш используется для быстрой предварительной проверки - если хэш не найден в базе, операция точно не является дубликатом. Если хэш найден, выполняется детальная проверка по полям для подтверждения дубликата, так как теоретически возможны коллизии хэшей.

Алгоритм сравнения использует три критерия с допусками. Дата операции сравнивается с окном ±2 дня - это учитывает возможные задержки в обработке банковских операций и разницу во времени между списанием и поступлением. Сумма должна совпадать точно (без округлений), так как банковские операции имеют фиксированные суммы. Описание платежа сравнивается по первым 50 символам - это позволяет игнорировать различия в конце длинных описаний, но сохраняет уникальность для разных операций. Все три критерия должны совпадать одновременно для признания операции дубликатом.

Проверка выполняется в двух местах для полноты. Сначала проверяется таблица `imported_operations` с фильтром `processed: false` - это позволяет обнаружить дубликаты среди необработанных черновиков из текущей или предыдущих сессий импорта. Затем проверяется таблица `operations` - это находит дубликаты среди уже импортированных операций, предотвращая повторный импорт. Проверка оптимизирована через фильтрацию по датам и суммам на уровне БД перед детальным сравнением в памяти.

При обнаружении дубликата операция помечается флагом `isDuplicate: true`, но не удаляется автоматически. Пользователь может просмотреть помеченные дубликаты в интерфейсе импорта и решить, импортировать ли их вручную (например, если это действительно разные операции с похожими параметрами). Система также подсчитывает общее количество дубликатов в файле и возвращает эту информацию при загрузке выписки, что позволяет пользователю оценить качество данных перед импортом.

**Ключевые файлы:** `duplicate-detection.service.ts` (метод `checkDuplicatesByHash`). Хэш создается через `createOperationHash` из `@fin-u-ch/shared/lib/operationHash`. Для изменения логики сравнения редактируйте условия в методе проверки дубликатов - можно изменить окно дат, длину сравнения описания или добавить дополнительные критерии. Для оптимизации при больших объемах данных можно добавить индексы на поля date и amount в таблицах operations и imported_operations.
